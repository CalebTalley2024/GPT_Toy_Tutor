{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-05-05T06:03:18.410268600Z",
     "start_time": "2024-05-05T06:03:10.656822400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# import custom python files for memory and students functions\n",
    "import memory\n",
    "import students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/GPT_tutor_topics(subtopics_included).csv')\n",
    "load_dotenv() # load up ENV variables\n",
    "# get key and model\n",
    "openai.api_key = os.getenv('OPENAI_KEY_1')\n",
    "model_35 = \"gpt-3.5-turbo\"\n",
    "question_temp = 1 # temperature of GPT, between 0 and 2\n",
    "math_df_path = '../data/GPT_tutor_topics(subtopics_included).csv'\n",
    "preprocessed_subtopics_DB = pd.read_csv( 'subtopics.csv')\n",
    "gpt_solve_time_placeholder = 30 # TODO find better place holder\n",
    "# df = pd.read_csv(math_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# prints horizontal line made of 'len' worth of dashes\n",
    "def print_line(len = 150):\n",
    "    if len == 0:\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"-\" * len)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T03:31:55.988445600Z",
     "start_time": "2024-05-06T03:31:55.974836800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# print_line()\n",
    "# takes in path\n",
    "# outputs (grade, education, topic, subtopic)\n",
    "def get_subtopic_math_data(path = math_df_path):\n",
    "    print_line()\n",
    "    print(\"Get The Subtopic:\\n\")\n",
    "    # math_df: dataframe with grade, education level, topic and subtopics\n",
    "    math_df = pd.read_csv(path)\n",
    "    grade = input(\"What grade are you in (Grade 1-12): \\n\")\n",
    "    while True:  # Loop until valid grade is entered\n",
    "        if 0 <= int(grade) <= 12 or grade.upper() == 'K':\n",
    "            break  # Exit loop if grade is valid\n",
    "        elif grade == \"\":\n",
    "            return -1\n",
    "        else:\n",
    "            grade = input(\"Invalid grade. Please enter a grade between 1-12': \")\n",
    "\n",
    "    education = -1\n",
    "    # figure out educations level\n",
    "    if grade in [ \"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "        education = \"Elementary\"\n",
    "    elif grade in [\"6\", \"7\", \"8\"]:\n",
    "        education = \"Middle School\"\n",
    "    elif grade in [\"9\", \"10\", \"11\", \"12\"]:\n",
    "        education = \"High School\"\n",
    "\n",
    "    print(f\"Grade {grade}, Education: {education}\")\n",
    "    \n",
    "    grade = int(grade)\n",
    "    # filter to get rows with that grade (.values converts df -> np array)\n",
    "    filt_df = math_df.loc[math_df[\"Grade\"] == grade].values # numpy array version\n",
    "    filt_df_topics = math_df.loc[math_df[\"Grade\"] == grade][\"Math Topic\"].values # numpy array version    \n",
    "\n",
    "    print(f\"\\nTopics for Grade {grade}, Education Level: {education}\")\n",
    "    \n",
    "    for i, topic_name in enumerate(filt_df_topics):\n",
    "        print(f\"{i}: {topic_name} \")\n",
    "    topic_idx = input(\"\\nPick a topic by number: \\n\")\n",
    "    while True:  # Loop for input validation\n",
    "        try:\n",
    "            topic_idx = int(topic_idx)\n",
    "            if 0 <= topic_idx < len(filt_df):  # Check if index is within range\n",
    "                break\n",
    "            elif topic_idx == \"\":\n",
    "                return -1\n",
    "            else:\n",
    "                topic_idx = input(\"Invalid number. Please enter a valid number from the list: \")\n",
    "        except ValueError:\n",
    "            topic_idx = input(\"Invalid input. Please enter a number: \")\n",
    "\n",
    "    topic_name = filt_df[topic_idx][2] # gets the name from the column\n",
    "    filt_df = filt_df[topic_idx][3:] # gets the row for the corresponding topic, remove rows for grade and education\n",
    "    \n",
    "    print(f\"\\nSubtopics for '{topic_name}'\")\n",
    "    \n",
    "    for i, subtopic_name in enumerate(filt_df): # go through columns\n",
    "        print(f\"{i}: {subtopic_name} \")\n",
    "\n",
    "    # subtopic_idx = input(\"Pick the number that matches your preferred subtopic: \\n\")\n",
    "\n",
    "    while True:\n",
    "        subtopic_idx = input(\"Pick the number that matches your preferred subtopic: \\n\")\n",
    "        try:\n",
    "            subtopic_idx = int(subtopic_idx)\n",
    "            if 0 <= subtopic_idx < len(filt_df): # in range\n",
    "                break\n",
    "            elif subtopic_idx == \"\":\n",
    "                return -2\n",
    "            else: # Error: out of range\n",
    "                input(\"Invalid input. Please enter a number between 0 and 4\")\n",
    "                \n",
    "        except ValueError: # Error: NaN\n",
    "            subtopic_idx = input(\"Invalid input. Please enter a number between 0 and 4\")\n",
    "            \n",
    "    subtopic_name = filt_df[subtopic_idx]\n",
    "    # make the id token a collection of all of the math data        \n",
    "    id_token = f\"{grade}|{education}|{topic_name}|{subtopic_name}\"\n",
    "\n",
    "    level = -1 # level does not have to be part of the ID, the subtopic obj hashmap deals with different levels\n",
    "    while level > 5 or level < 1: # continue the loop till we have a valid level\n",
    "        level = int(input(\"Enter the question level you want between 1 and 5: \\n\"))\n",
    "        if level > 5 or level < 1:\n",
    "            print(\"Invalid level, pick again.\")\n",
    "    return grade, education, topic_name, subtopic_name, id_token, level"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:27.667103500Z",
     "start_time": "2024-05-05T06:11:27.636521700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# get_subtopic_math_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:27.941183400Z",
     "start_time": "2024-05-05T06:11:27.896107800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:28.190985800Z",
     "start_time": "2024-05-05T06:11:28.162420700Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# API responses\n",
    "# be able to get the response and edit temperature\n",
    "def get_response(messages, temp = 0):\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model = model_35,\n",
    "        messages = messages,\n",
    "        temperature = temp # make sure responses are deterministic/consistent\n",
    "    )\n",
    "    return res\n",
    "# input: messages, temperature\n",
    "# returns response text\n",
    "def get_response_text(messages, temp = 0):\n",
    "    res = get_response(messages,temp)\n",
    "    return res['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:28.802082800Z",
     "start_time": "2024-05-05T06:11:28.766349700Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates one part of the message that you send to the GPT API for a response.\n",
    "# add brackets [] if you want to use this function to make a full message\n",
    "# System: 1\n",
    "# Assistant: 2\n",
    "# User: 3\n",
    "def create_message_part(text, role_type):\n",
    "    role = None\n",
    "    if role_type == 1:\n",
    "        role = \"system\"\n",
    "    elif role_type == 2:\n",
    "        role = \"assistant\"\n",
    "    elif role_type == 3:\n",
    "        role = \"user\"\n",
    "    message_part = {\n",
    "        \"role\": role,\n",
    "        \"content\": text\n",
    "    }\n",
    "    return message_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:29.076034900Z",
     "start_time": "2024-05-05T06:11:29.056495400Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper functions for ask_question\n",
    "# database is currently 'students.json'\n",
    "# 1. make sure GPT only answers math questions\n",
    "def filter_answers():\n",
    "    message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \n",
    "            f\"I am a math teacher for Grade K-12 in the United States. I am using the GPT API to help me answer my students' math questions. Please only answer my questions about math, and do not respond to any questions that are not about math.\"\n",
    "            \n",
    "            f\"Some math queries are wordy, such as Grade 12|High School|Exponential and logarithmic functions|Applications of exponential and logarithmic functions. Remember that this is also a math related query\"\n",
    "    }\n",
    "    return message\n",
    "\n",
    "# is_current_student: boolean\n",
    "def init_question(student_name, subtopic_obj, level):\n",
    "\n",
    "    # criteria: tell GPT scales for proficiency and level\n",
    "    init = f\"Based on {student_name}'s database, the student's skill level for {subtopic_obj.topic_name}, (specifically{subtopic_obj.name}) is {level}. Please give {student_name} a test question based on {subtopic_obj.topic_name}, (specifically{subtopic_obj.name}) and follow up with a sentence like 'Explain how you got your answer'. Adjust the difficulty of the question based on his skill level and proficiency score. DO NOT include any other words. Do not put the answer in the prompt.\"\n",
    "    criteria = f\"Level is on a scale between 1 and 5, where 5 is the hardest level.\"\n",
    "\n",
    "    # combine criteria and message\n",
    "    message = f\"{init} {criteria}\"\n",
    "    init_crit = create_message_part(message,1) # create system message\n",
    "    return init_crit\n",
    "\n",
    "# changes the format of the question GPT gives\n",
    "def question_formatting():\n",
    "    init = \"\"\"\n",
    "    This is the format that you should be using\n",
    "\n",
    "    \"\"\"\n",
    "    format = \"\"\"\n",
    "        Level 1 (Difficulty: Easy):\n",
    "        Subtract the following without regrouping (no borrowing):\n",
    "\n",
    "        1. 46-19\n",
    "\n",
    "    \"\"\"\n",
    "    level_meaning = \"\"\"\n",
    "    Remember the description that follows each Level\n",
    "\n",
    "    Level 1 (Difficulty: Easy):\n",
    "\n",
    "    Level 2 (Difficulty: Easy-Moderate):\n",
    "\n",
    "    Level 3 (Difficulty: Moderate):\n",
    "\n",
    "    Level 4 (Difficulty: Moderate-Hard):\n",
    "    Level 4 (Difficulty: Hard):\n",
    "\n",
    "    \"\"\"\n",
    "    formatting =  {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{init},{format}\"\n",
    "        }\n",
    "\n",
    "    level_meaning = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": level_meaning\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    return formatting, level_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# filter_answers()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:29.402306800Z",
     "start_time": "2024-05-05T06:11:29.367672200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:30.080446700Z",
     "start_time": "2024-05-05T06:11:30.069478200Z"
    }
   },
   "outputs": [],
   "source": [
    "# ask question to student\n",
    "# if in trainer mode, you can give feedback\n",
    "def ask_question(student_name, subtopic_obj, user_type, id_token,level):\n",
    "    print_line()\n",
    "    # print(\"Answer the Question:\\n\")\n",
    "    # make sure to only receive math answers and initialize the questions GPT will give\n",
    "    filter_subject = filter_answers()\n",
    "    filter_question = init_question(student_name, subtopic_obj,level) # level is picked here\n",
    "    formatting, level_meaning = question_formatting()\n",
    "\n",
    "   \n",
    "    tutor_question = generate_proposed_question(filter_subject, filter_question, formatting, level_meaning, user_type, id_token)    \n",
    "    \n",
    "    # here we print out the question GPT gives the student\n",
    "    # make sure the question is always in lower case\n",
    "    tutor_question = tutor_question.lower()\n",
    "    \n",
    "    if user_type == \"user\": # for trainer mode, the question has already been printed at this point ( b/c of Question making attempts)\n",
    "        print(f\"GPT's Question: \\n{tutor_question}: \\n\\n\")\n",
    "    # print_line()\n",
    "    \n",
    "    return tutor_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:30.398683Z",
     "start_time": "2024-05-05T06:11:30.372097Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a proposed_question\n",
    "# Mempropmt option! If you are a trainer, you can give feedback to GPT tutor to have it ask better questions\n",
    "# final question returned is what the user will use\n",
    "# id_token is the query\n",
    "def generate_proposed_question(filter_subject, filter_question, formatting, level_meaning, user_type, id_token, num_attempts = 0):\n",
    "    \n",
    "    # access Mempropmt question collection\n",
    "    question_coll = memory.MemPrompt().questions\n",
    "\n",
    "    similar_query, similar_feedback = question_coll.find_most_similar_memory(id_token) # get feedback, if None -> \" \"\n",
    "    similar_feedback_str = f\"{similar_feedback}\"\n",
    "    if user_type == \"trainer\":\n",
    "        print(f\"found feedback from MemPrompt: {similar_feedback_str}\\n\")\n",
    "    old_feedback_api_part = create_message_part(similar_feedback_str,1) # create system message\n",
    "    messages = [old_feedback_api_part, filter_subject, filter_question, formatting, level_meaning]\n",
    "    # print(messages)\n",
    "    # send the formatting to GPT and get a response\n",
    "    proposed_question = get_response_text(messages,question_temp)\n",
    "\n",
    "    if user_type == \"trainer\": # recursive call for gaining feedback \n",
    "        print_line()\n",
    "        print(f\"Proposed Question, Attempt: {num_attempts}\")\n",
    "        print_line()\n",
    "        given_new_feedback = question_coll.give_feedback(proposed_question, id_token)\n",
    "        # print_line()\n",
    "        \n",
    "        if given_new_feedback:\n",
    "            num_attempts += 1 # increment num_attempts\n",
    "            proposed_question = generate_proposed_question(filter_subject, filter_question, formatting, level_meaning, user_type, id_token,num_attempts)\n",
    "\n",
    "    # once the trainer is satisfied, or if the person is not a trainer,  return the question\n",
    "    return proposed_question \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:30.551271Z",
     "start_time": "2024-05-05T06:11:30.529475800Z"
    }
   },
   "outputs": [],
   "source": [
    "# ask_question(\"Alice\", \"2 digit division\",\"user\") #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:30.728457600Z",
     "start_time": "2024-05-05T06:11:30.705103800Z"
    }
   },
   "outputs": [],
   "source": [
    "# time: time it took the student to answer the question given from GPT\n",
    "# returns\n",
    "#   - the response the student gives\n",
    "#   - the time it takes to get a response form the student\n",
    "def get_student_timed_response():\n",
    "    start_time = time.time()\n",
    "\n",
    "    student_res = input() # response to question\n",
    "\n",
    "    end_time =  time.time()\n",
    "    end_time = end_time - start_time\n",
    "\n",
    "    return student_res, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compares student answer to GPT's answer, let student know what they got wrong\n",
    "# also considers student mistakes\n",
    "def respond_to_student_ans(question, student_answer, student_name, gpt_ans_explanation,get_all_student_related_mistakes, user_type):\n",
    "    # take in the student_name's answer, and the topic\n",
    "    print_line()\n",
    "    if user_type == \"trainer\":\n",
    "        print(f\"GPT's Answer: \\n {gpt_ans_explanation}\\n\") # show GPT's answer\n",
    "    \n",
    "    question_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a math tutor. The question that the user is answering is '{question}'.\"\n",
    "\n",
    "    }\n",
    "    answer_explained = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The GPT answer that you have found is derived below: \\n{gpt_ans_explanation}\\n\\n{student_name}'s answer is {student_answer}. Tell whether the student got the question correct based on the GPT answer and give and provide an explanation of the correct answer. Also explain where the student is incorrect\"\n",
    "    }\n",
    "\n",
    "    use_student_mistakes = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\" These are the mistakes that {student_name} made while doing these types of problems: {get_all_student_related_mistakes}. in your answer. highlight how he has improved on his mistakes,and/or how he is still doing the same mistake. If the student does not currently have any related mistakes, dont mention anything about related mistakes\"\n",
    "    }\n",
    "    init_response_messages = [question_message,answer_explained,use_student_mistakes]\n",
    "\n",
    "    answer_res = get_response_text(init_response_messages)\n",
    "\n",
    "    print(f\"GPT initial response to {student_name}'s answer: \\n{answer_res}\\n\")\n",
    "\n",
    "    return answer_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "# grades student response\n",
    "def grade_student_response(question, student_answer, student_name,solve_time, subtopic,answer_res, feedback):\n",
    "    print_line()\n",
    "    print(\"Evaluation on Student Performance: \\n\")\n",
    "    # if the question being asked is simple we want to make sure the user does not have to give an explanation if non is needed\n",
    "    # here are examples of questions that don't need  much explanation\n",
    "    simple_question_examples = \"\"\"\n",
    "    Addition: 2 + 3, 99 + 92\n",
    "    Subtraction: 10 - 4, 345 - 234\n",
    "    Multiplication: 5 * 6, 99 * 99\n",
    "    Division: 20 ÷ 4\n",
    "    Square of a number: 4**2, 78**2\n",
    "    Cube of a number: 3**3\n",
    "    \"\"\"\n",
    "\n",
    "    example_eval = \"\"\"\n",
    "        Evaluation of Allan's Performance:\n",
    "        \n",
    "        Question: What is 25 + 36\n",
    "        Student's Answer is Correct?: True\n",
    "        Level: 3\n",
    "        \n",
    "        1. Communication: 4/5\n",
    "           - Allan effectively communicated his answer and explanation in a clear and concise manner. However, there could have been more elaboration and clarity in his explanation.\n",
    "\n",
    "        2. Interpretation: 5/5\n",
    "           - Allan correctly interpreted the given equation and understood the objective of isolating x.\n",
    "\n",
    "        3. Computation: 5/5\n",
    "           - Allan correctly performed the necessary computation steps to solve the equation and obtained the correct answer.\n",
    "\n",
    "        4. Conceptual Understanding: 4/5\n",
    "           - Allan demonstrated a good understanding of the concept of isolating x in an equation. However, his explanation could have included more conceptual details to further enhance his understanding.\n",
    "\n",
    "        5. Time Taken: 5/5\n",
    "           - Allan was able to solve the question in a relatively short amount of time, taking only 20 seconds.\n",
    "\n",
    "        Average Score: (4 + 5 + 5 + 4 + 5) / 5 = 4.6/5\n",
    "\n",
    "        Explanation:\n",
    "        Allan's performance was generally strong across all skill metrics. He effectively communicated his answer and demonstrated a good understanding of the concept. However, his explanation could have been more detailed and comprehensive, which affected his score in the communication and conceptual understanding categories. Overall, Allan performed well and achieved a high average score of 4.6 out of 5.\n",
    "        \"\"\"\n",
    "    evaluation_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"The topic of the question is {subtopic}. This is the question given to {student_name}: {question}. {student_name}'s answer is {student_answer}. This  is the answer you gave: {answer_res}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"I need you to evaluate {student_name}'s performance in terms of the following skill metrics: communication, interpretation, computation, conceptual, and the time taken to solve the question (it took the student {solve_time} seconds to complete the question. For each of these metrics, rate the skill out of 5, where 5 out of 5 is the best score, and 1 out of 5 is the worst score. make sure to have your evaluation in outline format. Also give an explanation on how {student_name} did not get the highest marks. Make sure to use Integers, NOT decimals \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"if the question being asked is brief and does not require much of an explanation, automatically give the student a 5 out of 5 score for communication. The following are examples of questions that dont require the student to give a long explanation: {simple_question_examples}. for all questions similar to this, the metric score for communication will always be 5.\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \" if you cannot find any related mistakes for a metric, automatically give a score of 5 for the corresponding metric\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"at the end, give an average score based on the above metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"make sure that you always have a time metric\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Make sure to display the Question first in the following format (<question> = the question asked): \\n Question: <question>\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Example response:{example_eval} \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\" remember the following when making the Evaluation: {feedback} \"\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "    evaluation_res = get_response_text(evaluation_messages)\n",
    "    print(evaluation_res)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return evaluation_res\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-05T06:11:30.900206800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T04:03:54.611846900Z",
     "start_time": "2024-05-06T04:03:54.606218300Z"
    }
   },
   "outputs": [],
   "source": [
    "# give students the ability to ask for clarification regarding a question they have about the answer to the question\n",
    "def get_gpt_clarification (question, gpt_answer, student_answer, previous_explanations):\n",
    "    # ask the student if they need clarification on a question.\n",
    "    # if they do. give them a chance to ask a question about GPT's Answer\n",
    "    # if they don't, then don't provide anything\n",
    "    # make sure the response safe and only related to mathematics\n",
    "    only_answer_math = f'''I am a math teacher for Grade K-12 in the United States. I am using the GPT API to help me answer my students' math questions. Please only answer my questions about math, and do not respond to any questions that are not about math.'''\n",
    "\n",
    "    only_answer_math_msg = create_message_part(only_answer_math,1)\n",
    "\n",
    "    previous_info = f\"\"\"\n",
    "    Question: {question},\n",
    "    GPT's answer ( assume this is correct): {gpt_answer},\n",
    "    student's answer (this could, or could not be correct){student_answer},\n",
    "    previous explanations to students questions:{previous_explanations}\n",
    "    \"\"\"\n",
    "    # create system message\n",
    "    previous_info_msg = create_message_part(previous_info,1)\n",
    "\n",
    "\n",
    "    # create message for the student's question on the math problem\n",
    "    student_question = input(\"Write down what you want clarification on: \\n\")\n",
    "    print(0) # empty line\n",
    "    student_question_msg = create_message_part(student_question,3)\n",
    "\n",
    "    msgs = [only_answer_math_msg,previous_info_msg,student_question_msg]\n",
    "\n",
    "    GPT_res = get_response_text(msgs)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(GPT_res)\n",
    "\n",
    "    return GPT_res\n",
    "\n",
    "# GPT gives a detailed explanation given a student's question on the math problem\n",
    "# ques_explain: math question and the previous student questions asked and the GPT resposes given\n",
    "def student_clarification(question, gpt_answer, student_answer, previous_explanations):\n",
    "    # Ask the student if they want clarification about the answers given\n",
    "    need_clarification = input(\"If you want clarification, type 'Yes'. Type anything else to got to the evaluation section\\n\")\n",
    "    print_line(0)\n",
    "    need_clarification = need_clarification.lower()\n",
    "    if need_clarification == \"yes\".lower():\n",
    "        # Get clarification from the student using the get_gpt_clarification function\n",
    "        new_clarification = get_gpt_clarification(question, gpt_answer, student_answer, previous_explanations)\n",
    "\n",
    "        # Update previous explanations with the new clarification\n",
    "        previous_explanations = previous_explanations + \"  ,  \" + new_clarification\n",
    "\n",
    "        # Recursive call to continue the clarification process\n",
    "        student_clarification(question, gpt_answer, student_answer, previous_explanations)\n",
    "    else:\n",
    "        print(\"Student questioning section has been completed.\\nNext: Metric scores for performance\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T04:36:37.139322Z",
     "start_time": "2024-05-06T04:36:37.121131500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# gpt_res: evaluation on how the student answered the questions\n",
    "#TODO Suggestion: add questions to examples and json\n",
    "#TODO Suggestion: add if the answer is correct\n",
    "#TODO Suggestion: figure out eval() error from GPT not formatting JSON correctly\n",
    "def extract_metrics_scores(gpt_res):\n",
    "    # print(gpt_res)\n",
    "    instruction = f'''\n",
    "\n",
    "\n",
    "    Here is the Evaluation for a certain student. \\n\\n{gpt_res}\\n\\n---\\n.\n",
    "\n",
    "    From this evaluation, extract it's evaluation metric numbers and put them in the shape of a JSON file.  \n",
    "    - question\n",
    "    - subtopic name\n",
    "    - level\n",
    "    - communication\n",
    "    - computation\n",
    "    - conceptual\n",
    "    - interpretation\n",
    "    - mistakes (NOT an array, this should just be one single string, with each mistake separated by commas)\n",
    "    - overall_avg\n",
    "    - time (score and seconds)\n",
    "    \n",
    "    Also Include the question asked\n",
    "    \n",
    "    Answer this question in the form of a JSON file.\n",
    "    \n",
    "    Please respond in plain text without using any code formatting like \n",
    "    DO NOT USE THE FOLLOWING\n",
    "     - 'json', \n",
    "     - '```', \n",
    "     - <string>, etc. \n",
    "     DONT FORGET TO \n",
    "     - use commas when necessary\n",
    "     - MAKE SURE JSON is formatted perfectly, such that I can use eval.\n",
    "     \n",
    "     Look at the 2 examples provided below. For each one...\n",
    "     - I first give sample input\n",
    "     - I then give you what you should output. Notice that the output has perfect Format for creating a JSON\n",
    "'''\n",
    "    # example input\n",
    "    example_1_eval = \"\"\"\n",
    "        Evaluation of Allan's Performance:\n",
    "        \n",
    "        Question: Add the following numbers: find x: 3x + 4 = 31\n",
    "        Subtopic: Basic Algebra\n",
    "        Level: 3\n",
    "        \n",
    "        1. Communication: 4/5\n",
    "           - Allan effectively communicated his answer and explanation in a clear and concise manner. However, there could have been more elaboration and clarity in his explanation.\n",
    "\n",
    "        2. Interpretation: 5/5\n",
    "           - Allan correctly interpreted the given equation and understood the objective of isolating x.\n",
    "\n",
    "        3. Computation: 5/5\n",
    "           - Allan correctly performed the necessary computation steps to solve the equation and obtained the correct answer.\n",
    "\n",
    "        4. Conceptual Understanding: 4/5\n",
    "           - Allan demonstrated a good understanding of the concept of isolating x in an equation. However, his explanation could have included more conceptual details to further enhance his understanding.\n",
    "\n",
    "        5. Time Taken: 5/5\n",
    "           - Allan was able to solve the question in a relatively short amount of time, taking only 20 seconds.\n",
    "\n",
    "        Average Score: (4 + 5 + 5 + 4 + 5) / 5 = 4.6/5\n",
    "\n",
    "        Explanation:\n",
    "        Allan's performance was generally strong across all skill metrics. He effectively communicated his answer and demonstrated a good understanding of the concept. However, his explanation could have been more detailed and comprehensive, which affected his score in the communication and conceptual understanding categories. Overall, Allan performed well and achieved a high average score of 4.6 out of 5.\n",
    "        \"\"\"\n",
    "\n",
    "    example_1_res = \"\"\"{ \n",
    "    \"question\": \"find x: 3x + 4 = 31\",\n",
    "    \"subtopic\":  \"basic algebra\",\n",
    "    \"level\": 3,\n",
    "    \"overall_avg\": 4.6,\n",
    "    \"communication\": {\n",
    "        \"score\": 4\n",
    "    },\n",
    "    \"interpretation\": {\n",
    "        \"score\": 5\n",
    "    },\n",
    "    \"computation\": {\n",
    "        \"score\": 5\n",
    "    },\n",
    "    \"conceptual\": {\n",
    "        \"score\": 4\n",
    "    },\n",
    "    \"time\": {\n",
    "        \"score\": 5,\n",
    "        \"seconds\": 20\n",
    "    },\n",
    "    \"mistakes\": \n",
    "        \"could have been more elaboration and clarity in his explanation,\n",
    "        explanation could have included more conceptual details to further enhance his understanding\"\n",
    "    \n",
    "}\"\"\"\n",
    "\n",
    "    example_2_eval = \"\"\"\n",
    "        Evaluation of Alice's Performance:\n",
    "\n",
    "    Question: A bakery has 86 cupcakes. They sell 59 cupcakes. How many cupcakes do they have left? Solve this without regrouping (borrowing). Show your work.\n",
    "\n",
    "    Level 2\n",
    "    \n",
    "    1. Communication: 2/5\n",
    "       - Alice's answer does not clearly explain the steps taken to subtract the numbers.\n",
    "       - The answer provided is incorrect and does not demonstrate a clear understanding of the subtraction process.\n",
    "\n",
    "    2. Interpretation: 1/5\n",
    "       - Alice misinterpreted the question and did not understand that regrouping (borrowing) was not allowed.\n",
    "       - The answer provided does not align with the given instructions.\n",
    "\n",
    "    3. Computation: 1/5\n",
    "       - Alice's answer of 24 is incorrect and does not reflect the correct subtraction calculation.\n",
    "       - The computation process used by Alice is flawed and does not follow the correct method of subtraction without regrouping.\n",
    "\n",
    "    4. Conceptual: 1/5\n",
    "       - Alice lacks a clear understanding of the concept of subtraction without regrouping.\n",
    "       - The incorrect answer and flawed computation process indicate a lack of conceptual understanding.\n",
    "\n",
    "    5. Time taken: 5/5\n",
    "       - Alice completed the question in a reasonable amount of time, indicating a decent level of speed and efficiency.\n",
    "\n",
    "    Average Score: (2 + 1 + 1 + 1 + 5) / 5 = 2/5\n",
    "\n",
    "    Explanation:\n",
    "    Alice's performance in this question is below average. She struggled with communication, interpretation, computation, and conceptual understanding. Her answer was incorrect, and she did not follow the correct method of subtraction without regrouping. Alice needs further practice and clarification on the concept of subtraction to improve her skills in this area.\n",
    "    \"\"\"\n",
    "    # mistakes is a string, with commas separating different mistakes, NOT an array\n",
    "    example_2_res = \"\"\" {\n",
    "        \"question\": \"A bakery has 86 cupcakes. They sell 59 cupcakes. How many cupcakes do they have left? Solve this without regrouping (borrowing). Show your work.\",\n",
    "        \"subtopic\": \"word problem without regrouping\",\n",
    "        \"level\": 2,\n",
    "        \"overall_avg\": 2,\n",
    "        \"communication\": {\n",
    "            \"score\": 2,\n",
    "        },\n",
    "        \"interpretation\": {\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        \"computation\": {\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        \"conceptual\": {\n",
    "            \"score\": 1,\n",
    "        },\n",
    "        \"time\": {\n",
    "            \"score\": 5,\n",
    "            \"seconds\": 23\n",
    "        },\n",
    "        \"mistakes\":\n",
    "        \"does not clearly explain the steps taken to subtract the numbers,\n",
    "        misinterpreted the question and did not understand that regrouping (borrowing) was not allowed,\n",
    "        The answer provided does not align with the given instructions,\n",
    "        does not follow the correct method of subtraction without regrouping,\n",
    "        lacks a clear understanding of the concept of subtraction without regrouping\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    instruction_msg = create_message_part(instruction, 1)\n",
    "    example_1_res_msg = create_message_part(example_1_eval, 3)\n",
    "    example_1_res_ans = create_message_part(example_1_res, 2)\n",
    "    example_2_res_msg = create_message_part(example_2_eval, 3)\n",
    "    example_2_res_ans = create_message_part(example_2_res, 2)\n",
    "\n",
    "    # add instructions and examples to messages\n",
    "    messages = [instruction_msg, example_1_res_msg, example_1_res_ans, example_2_res_msg, example_2_res_ans]\n",
    "\n",
    "    # get JSON data in form of response string\n",
    "    metric_scores_string = get_response_text(messages, 0.3)  # temp  = 0.3\n",
    "\n",
    "    return metric_scores_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:32.397848Z",
     "start_time": "2024-05-05T06:11:32.376105900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# receives students answer, and returns GPT's evaluations of their answer compared to GPT's answer\n",
    "def receive_and_evaluate(question, student, subtopic, student_answer_explanation, gpt_ans_explanation, user_type = \"user\", num_attempts = 0, solve_time = gpt_solve_time_placeholder):\n",
    "    \n",
    "    # if user_type == \"trainer\":\n",
    "    #     solve_time = solve_time\n",
    "    # Grade the student's response using the given question, student response, time, and subtopic\n",
    "\n",
    "    # get the answer + explanation that GPT provides with python doing the math.\n",
    "    # uses \"memPrompt\" like memory\n",
    "    answer_res = respond_to_student_ans(question, student_answer_explanation, student.name, gpt_ans_explanation,student.mistakes, user_type) # this also prints out GPT's final answer\n",
    "    \n",
    "    eval_coll = memory.MemPrompt().evaluations\n",
    "\n",
    "    # get old feedback\n",
    "    _, similar_feedback = eval_coll.find_most_similar_memory(answer_res) # get feedback, if None -> \" \"\n",
    "    similar_feedback_str = f\"{similar_feedback}\"\n",
    "    if user_type == \"trainer\":\n",
    "        print(f\"found feedback from MemPrompt: {similar_feedback_str}\\n\")\n",
    "    \n",
    "    previous_explanations = \" \" # string is initially empty b/c the student has not asked any clarifying questions yet\n",
    "    if user_type == \"user\":\n",
    "        student_clarification(question,answer_res,student_answer_explanation,previous_explanations) \n",
    "    gpt_eval_res = grade_student_response(question, student_answer_explanation, student.name, solve_time, subtopic.name,answer_res,similar_feedback)\n",
    "    \n",
    "    if user_type == \"trainer\":  # recursive call for gaining feedback \n",
    "        print_line()\n",
    "        print(f\"GPT Answer + Explanation Evaluation: Attempt {num_attempts}\")\n",
    "        print_line()\n",
    "        # get evaluation collection from database\n",
    "        \n",
    "        given_new_feedback = eval_coll.give_feedback(question,student_answer_explanation,answer_res,gpt_eval_res)\n",
    "        # print_line()\n",
    "        \n",
    "        if given_new_feedback:\n",
    "            num_attempts += 1\n",
    "            gpt_eval_res = receive_and_evaluate(question, student, subtopic, student_answer_explanation, gpt_ans_explanation,\"trainer\", num_attempts)  \n",
    "        \n",
    "    return gpt_eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# question: String\n",
    "# student, subtopic: custom Objects\n",
    "# all_student_subtopic_mistakes: dictionary\n",
    "# receive student's answer, respond to their answer, and update their statistics\n",
    "# returns metric updates, non_formatted GPT evaluation of student, and the solve time\n",
    "def receive_respond_and_update(question, student, subtopic, user_type = \"user\"):\n",
    "    # Get the student's response and the time taken\n",
    "    student_answer, solve_time = get_student_timed_response()\n",
    "\n",
    "    gpt_ans_explanation, _ = get_answer_explanation_with_memory(question)\n",
    "\n",
    "    gpt_eval_res = receive_and_evaluate(question,student,subtopic, student_answer,gpt_ans_explanation, user_type, _,  solve_time)\n",
    "\n",
    "    max_eval_attempts = 10 # if GPT messes up json format, the system can try another 9 times\n",
    "    for attempt in range(max_eval_attempts):\n",
    "        try:\n",
    "            # Extract metric updates from the GPT response\n",
    "            metric_updates_string = extract_metrics_scores(gpt_eval_res)\n",
    "            metric_updates = eval(metric_updates_string) # string --> json\n",
    "            # Return the metric updates\n",
    "            return metric_updates\n",
    "        except (json.JSONDecodeError, SyntaxError) as e:\n",
    "            if attempt == max_eval_attempts - 1:\n",
    "                raise ValueError(f\"Failed to parse metric scores JSON after {max_eval_attempts} attempts: {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Failed to parse metric scores JSON on attempt {attempt+1}. Retrying...\")\n",
    "\n",
    "    # Should not reach here if attempts are successful\n",
    "    # print_line()\n",
    "    return -1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:32.636194100Z",
     "start_time": "2024-05-05T06:11:32.612135200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:11:33.124038300Z",
     "start_time": "2024-05-05T06:11:33.088873800Z"
    }
   },
   "outputs": [],
   "source": [
    "# converts question's format into python formatting\n",
    "def question_python_notation(question):\n",
    "    system_text = \"\"\"\n",
    "    make the following questions in python notation:\n",
    "\n",
    "    Here is an example\"\n",
    "\n",
    "    User: \"What is 0.9^6\" in python notation\n",
    "\n",
    "    Assistant: \"What is 0.9**6\"\n",
    "\n",
    "    User: \"What is 0.9 raised to the power of 6\" in python notation\n",
    "\n",
    "    Assistant: \"What is 0.9**6\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    prompt_text = question\n",
    "    system_msg = create_message_part(system_text,1)\n",
    "    prompt_msg = create_message_part(prompt_text,3)\n",
    "    messages = [system_msg,prompt_msg]\n",
    "    converted_notation = get_response_text(messages)\n",
    "    # print( converted_notation)\n",
    "    return  converted_notation\n",
    "\n",
    "# using question given and answer given in python, make GPT backtrack to find a solution that gets the answer\n",
    "def backtrack_to_explanation(question, answer):\n",
    "    # to make sure that are answers are accurate and consistant, we make sure that GPT receives the answers in python notation\n",
    "    converted_question = question_python_notation(question)\n",
    "\n",
    "    system_text = \"You will be given a question, and an answer to a question. It is your job to explain the correct way how to get from the question to the answer step by step.  \"\n",
    "    prompt_text = f\"the question is {converted_question}, the answer is {answer}\"\n",
    "\n",
    "    system_msg = create_message_part(system_text,1)\n",
    "    prompt_msg = create_message_part(prompt_text,3)\n",
    "\n",
    "    messages = [system_msg,prompt_msg]\n",
    "    explanation = get_response_text(messages)\n",
    "    # print(explanation)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:20:59.557074400Z",
     "start_time": "2024-05-05T06:20:59.538924800Z"
    }
   },
   "outputs": [],
   "source": [
    "#### TRYING TO run python code to get question right\n",
    "def question_to_code_block(question):\n",
    "    message = [\n",
    "        {\n",
    "            # the code needed for \"exec\" to work need specific formatting\n",
    "            # make sure GPT will only give answers that are executable in python\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            When asked anything, the answer will always be in python code, using a function. no other comments. make sure to call the function. NEVER use ```python```\n",
    "\n",
    "            Example:\n",
    "\n",
    "            user: what is 0.9**6\n",
    "\n",
    "            assistant:\n",
    "\n",
    "            def test()\n",
    "                value = 0.9**6\n",
    "                ans = f\"the answer to the question is {value}\"\n",
    "                return ans\n",
    "            result = test() # MAKE SURE TO INCLUDE THE 'result = <function()>'\n",
    "            \"\"\"\n",
    "            # \"When asked anything, the answer will always be in python code, using a function. no other comments. use a print statement when calling the function\"\n",
    "            # \"When asked anything, the answer will always be in python code, using a function. no other comments, You are allowed to use external libraries such as numpy, scipy, etc\"\n",
    "\n",
    "            # \"When asked anything, the answer will always be in python code. no other comments, just python code. make sure the return is in a print statement\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    code_block = get_response_text(message)\n",
    "    return code_block\n",
    "# This function takes a code block as input and returns the value given when the python code is executed\n",
    "\n",
    "# the use of exec could possible be a security risk, but should not be here\n",
    "def code_block_to_variable(code_block):\n",
    "    # Create an empty dictionary to store the variables that are created in the executed code.\n",
    "    data = {}\n",
    "\n",
    "    # Execute the code block and store the results in the dictionary `data`.\n",
    "    exec(code_block, None, data)\n",
    "\n",
    "    # Get the value of the variable `result` from the dictionary `data`.\n",
    "    var = data[\"result\"]\n",
    "\n",
    "    return var\n",
    "\n",
    "def solve_math(question, user_type = \"user\"): \n",
    "    code_block = question_to_code_block(question)\n",
    "    if user_type == \"trainer\":\n",
    "        print_line()\n",
    "        print(f\"python code: {code_block}\")\n",
    "    ans = code_block_to_variable(code_block)\n",
    "    # convert answer into string\n",
    "    # print(f\"answer: {ans}\")\n",
    "    \n",
    "    explanation = backtrack_to_explanation(question,ans)\n",
    "    # print(f\"Explanation: {explanation}\")\n",
    "    return explanation, ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T04:25:53.445560Z",
     "start_time": "2024-05-01T04:25:53.435051600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# creates prompt with question and feedback if it exists\n",
    "def create_prompt(question, feedback):\n",
    "    # make the question and feedback into one prompt\n",
    "    question_msg = create_message_part(question,3)\n",
    "    set_up_msg = \"user feedback:\"\n",
    "    prepare_for_feedback = create_message_part(set_up_msg,3)\n",
    "\n",
    "    # feedback is given by the system\n",
    "    feedback_msg = create_message_part(feedback,1)\n",
    "\n",
    "    show_explanation = \"make sure to explain how you got to your answer\"\n",
    "    show_explanation_msg = create_message_part(show_explanation,1)\n",
    "\n",
    "    prompt = [question_msg, prepare_for_feedback, feedback_msg,show_explanation_msg]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# explanation(has answer) -> answer\n",
    "def get_answer_from_explanation(explained_ans):\n",
    "    instruction = \"From the given explanation, give only the question solved and the answer given\"\n",
    "    instruction_msg = create_message_part(instruction,1)\n",
    "    explained_ans_msg = create_message_part(explained_ans,3)\n",
    "\n",
    "    msgs = [instruction_msg, explained_ans_msg]\n",
    "    ans = get_response_text(msgs)\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_answer_and_explanation_old(prompt):\n",
    "#     # generate the explained answer\n",
    "#     explanation = get_response_text(prompt)\n",
    "#     answer = get_answer_from_explanation(explanation)\n",
    "#     return explanation, answer\n",
    "\n",
    "# splits up explained answer into two parts: The answer itself and the explanation that leads to the answer\n",
    "def get_answer_and_explanation(question_w_feedback, user_type = \"user\"):\n",
    "    # generate the explained answer and explanation\n",
    "    explanation, answer = solve_math(question_w_feedback)\n",
    "    return explanation, answer\n",
    "\n",
    "# will update the answer memory if the user spots a mistake that GPT has made in the answer and/or the explanation of the answer\n",
    "def update_ans_memory(question, answer, explanation):\n",
    "    # first display the answer to the user\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Explanation: {explanation}\\n\")\n",
    "    need_feedback = input(\"does the answer and explanation above require any feedback: 'Yes', or 'No': \\n\")\n",
    "    if need_feedback == 'Yes':\n",
    "        feedback = input(\"What needs to be improved in the analysis process?\")\n",
    "        # get answer collection from database\n",
    "        ans_coll = memory.MemPrompt().answers\n",
    "        ans_coll.update_memory_feedback(question, feedback)\n",
    "    else:\n",
    "        print(\"memory will not be updated\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T06:41:27.259395900Z",
     "start_time": "2024-05-05T06:41:27.235068500Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPT answer the question with the feedback memory json\n",
    "def get_answer_explanation_with_memory(question, num_attempts = 0, user_type = \"user\"):\n",
    "    \n",
    "    if num_attempts == 0:\n",
    "        \"Starting Answer Generation\"\n",
    "        # print_line()\n",
    "    if user_type == \"trainer\":\n",
    "        print_line()\n",
    "        print(f\"Generate Answer: Attempt {num_attempts} \\n\")\n",
    "        print_line()\n",
    "    # get the answer collection from database\n",
    "    ans_coll = memory.MemPrompt().answers\n",
    "    # get the feedback associated with the most similar question\n",
    "    _,similar_feedback = ans_coll.find_most_similar_memory(question) \n",
    "    # convert the list to a string    \n",
    "    # print(type(feedback_str))\n",
    "    \n",
    "    feedback_str = f\"{similar_feedback}\"\n",
    "    if user_type == \"trainer\":\n",
    "        print(f\"found feedback from MemPrompt: {feedback_str}\\n\")\n",
    "    \n",
    "    question_w_feedback = f\"{feedback_str, question}\"\n",
    "    # prompt = create_prompt(question,feedback_str)\n",
    "    # get the GPT generated answer and explanation\n",
    "    # ans_explanation: gives both the answer to the question and the explanation of the answer\n",
    "    proposed_ans_explanation, proposed_ans = get_answer_and_explanation(question_w_feedback, user_type)\n",
    "    \n",
    "    if user_type == \"trainer\": # recursive call for gaining feedback \n",
    "        given_new_feedback = ans_coll.give_feedback(question,proposed_ans,proposed_ans_explanation)\n",
    "        if given_new_feedback: # recursively call the function till the answer enough feedback is given to make the answer + explanation appropriate\n",
    "            num_attempts += 1\n",
    "            proposed_ans_explanation, proposed_ans = get_answer_explanation_with_memory(question, num_attempts, \"trainer\")\n",
    "        \n",
    "    # return final answer and explanation\n",
    "    return proposed_ans_explanation,proposed_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# recommends question based on student data\n",
    "def auto_select_subtopic_opt(student):\n",
    "    \n",
    "    student_data_str = json.dumps(student.to_json())\n",
    "    # weakness -->  find Topic/Subtopic Pair\n",
    "    # Pair --> Select Lvl\n",
    "    # Pair, Lvl --> Produce Question\n",
    "    # student info --> weakness/ no weaknesses\n",
    "    str_line = \"-\" * 25 # line to make distinction from data and next sentence\n",
    "    detect_weakness_str = f\"This is the {student.name}'s data: {student_data_str }\\n\\n{str_line}. THere is Subtopics data and Mistakes data. Let me know if there are any mistakes and imperfect scores in my data. \\n a Perfect Score = 5, Perfect Mistakes = '' \"\n",
    "    detect_weakness_msg = create_message_part(detect_weakness_str,3)\n",
    "    weakness_messages = [detect_weakness_msg]\n",
    "    get_all_low_score_subtopics_txt = get_response_text(weakness_messages,0)\n",
    "\n",
    "\n",
    "    subtopic_DB_txt =  f\"Subtopics Database (format = grade | education | topic_name | subtopic name): {preprocessed_subtopics_DB}\"\n",
    "\n",
    "    pick_topic_subtopic_pair_str = f\"Here is all of the students data: {student_data_str}, Here are the students weak subtopics in detail: {get_all_low_score_subtopics_txt}. Based on his weak points and the database I gave above, Give me a sub topic in the following format: 'grade | education | topic_name | subtopic name| \\n{str_line}. ONLY give me the subtopic ID in the form of 'grade | education | topic_name | subtopic name|', NOTHING ELSE\"\n",
    "\n",
    "    subtopic_DB_msg = create_message_part(subtopic_DB_txt,1)\n",
    "\n",
    "    pick_topic_subtopic_msg = create_message_part(pick_topic_subtopic_pair_str,3)\n",
    "\n",
    "    pick_subtopic_messages = [subtopic_DB_msg,pick_topic_subtopic_msg]\n",
    "\n",
    "    picked_subtopic_id = get_response_text(pick_subtopic_messages,0)\n",
    "\n",
    "\n",
    "\n",
    "    pick_level_str = f\"This is the subtopic ID that you picked: {picked_subtopic_id}. These are my weaknesses: {get_all_low_score_subtopics_txt}. based on the subtopic and my previous weaknesses determine the best level of a question that I get for this subtopic. The level should be between 1 - 5, 1 being easiest and 5 being the hardest. ONLY  give me the number, NOTHING else\"\n",
    "\n",
    "    pick_level_msg = create_message_part(pick_level_str,3)\n",
    "    picked_level = get_response_text([pick_level_msg],0)\n",
    "\n",
    "    # subtopic ID ---> array (grade, education, topic, subtopic)\n",
    "    subtopic_id_list = picked_subtopic_id.split('|')\n",
    "    # print(subtopic_id_list)\n",
    "    grade = subtopic_id_list[0]\n",
    "    education = subtopic_id_list[1]\n",
    "    topic_name = subtopic_id_list[2]\n",
    "    subtopic_name = subtopic_id_list[3]\n",
    "\n",
    "    return grade,education, topic_name, subtopic_name, picked_subtopic_id, picked_level"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-01T04:25:53.630223400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T04:25:53.910566Z",
     "start_time": "2024-05-01T04:25:53.890992900Z"
    }
   },
   "outputs": [],
   "source": [
    "# based off \"MemPrompt: memory-assisted Prompt Editing with User Feedback\" paper\n",
    "def mem_prompt_learning():\n",
    "    user_type = \"trainer\"\n",
    "    # get subtopic specific info \n",
    "    # id_token contains all of the info, separated by ': 's\n",
    "    grade, education, topic_name, subtopic_name, id_token, diff_level = get_subtopic_math_data(math_df_path)\n",
    "    # get memprompt collections of memory data\n",
    "    # this database has 3 collections: 'questions', 'answers', and 'evaluation'\n",
    "    \n",
    "    # make subtopic and student objects we will use just for training\n",
    "    # nothing will change in the students database collection on MongoDB\n",
    "    subtopic_placeholder = students.Subtopic(subtopic_name, grade, education, topic_name)\n",
    "    student_placeholder = students.Student(\"trainer\") # placeholder for a student's name. This will NOT negatively affect the ask_question function\n",
    "    \n",
    "    # get the question\n",
    "    question = ask_question(student_placeholder,subtopic_placeholder,user_type, id_token,diff_level)\n",
    "    # find the question, or the most similar question that's in the database already\n",
    "    # get the GPT generated answer and explanation\n",
    "    explanation, answer = get_answer_explanation_with_memory(question,0,\"trainer\")\n",
    "\n",
    "    # update the memory.json file ( if the answer is already correct, then nothing in the database will change)\n",
    "    # update_ans_memory(question, answer, explanation)\n",
    "    # perform evaluation\n",
    "\n",
    "    receive_and_evaluate(question, student_placeholder, subtopic_placeholder, answer, explanation,\"trainer\")    \n",
    "    # eval_coll = memprompt.evaluations\n",
    "    # eval_coll.give_feedback(question, answer,explanation,gpt_eval)\n",
    "\n",
    "    # Ask the student if they want to be asked another question.\n",
    "    answer = input(\"Do you want another question? 'yes' or 'no' \")\n",
    "    answer = answer.lower()\n",
    "    while answer not in (\"yes\", \"no\"):\n",
    "        answer = input(\"Invalid input: Please enter 'yes' or 'no': \")\n",
    "    if answer == \"yes\":\n",
    "        mem_prompt_learning()\n",
    "    else:\n",
    "        print(\"Thank you training GPT Tutor! Have a great day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T04:25:54.130480400Z",
     "start_time": "2024-05-01T04:25:54.112075Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# asks student question, evaluates and updates their database\n",
    "def student_learning():\n",
    "    user_type = \"user\"\n",
    "    # get students database collection from MongoDB\n",
    "    main_collection = \"Section0\"\n",
    "    # students.py is an import\n",
    "    StudentCollection = students.StudentsCollection(main_collection) \n",
    "    all_student_names = StudentCollection.current_student_names()\n",
    "\n",
    "    \"\"\"Asks the student a question and updates their stats.\"\"\"    \n",
    "    # get the student\n",
    "    full_name = input(\"Enter your full name (Example: John Doe): \\n\")\n",
    "   \n",
    "    # if student is in database, get the object\n",
    "    if full_name in all_student_names:\n",
    "        print(f\"{full_name} is already in the database\")\n",
    "        student = StudentCollection.get_student(full_name)\n",
    "    else:\n",
    "        # if the student is not in the database, create the object\n",
    "        print(f\"{full_name} will be added to the database\")\n",
    "        student = students.Student(full_name) # initialize student\n",
    "\n",
    "    subtopic_selection_type = -1\n",
    "    \n",
    "    # check if the subtopic array of the student obj is empty\n",
    "    empty_subtopic_array = False\n",
    "    if not student.subtopics: # if student.subtopics == []\n",
    "        empty_subtopic_array = True\n",
    "        \n",
    "        \n",
    "    if empty_subtopic_array:\n",
    "        print(\"This is your first question, so you will have to choose the Subtopic, Topic, etc\\n\")\n",
    "        grade, education, topic_name, subtopic_name, id_token, diff_level = get_subtopic_math_data(math_df_path)\n",
    "    else:\n",
    "        \n",
    "        while subtopic_selection_type not in {'0','1'}:\n",
    "            subtopic_selection_type = input(\"Type '0' for manual subtopic selection \\nType '1' for auto subtopic selection\\n\")\n",
    "            if subtopic_selection_type == '0':\n",
    "                print(\"Manual Option Selected\\n\")\n",
    "                grade, education, topic_name, subtopic_name, id_token, diff_level = get_subtopic_math_data(math_df_path)\n",
    "            elif subtopic_selection_type == '1':\n",
    "                print(\"Auto Option Selected\\n\")\n",
    "                grade, education, topic_name, subtopic_name, id_token, diff_level = auto_select_subtopic_opt(student ) \n",
    "            else:\n",
    "                print(\"try again, only '0' and '1' are valid \\n\")\n",
    "        \n",
    "    \n",
    "    # pick the grade, education, topic_name, subtopic_name, id_token, and the difficulty level\n",
    "    \n",
    "    \n",
    "    # find/create subtopic\n",
    "    if student.current_subtopic_ids(): # if current/student_names != NULL\n",
    "        if id_token in student.current_subtopic_ids():\n",
    "            subtopic = student.get_subtopic(id_token)\n",
    "        else:\n",
    "            subtopic = students.Subtopic(subtopic_name, grade, education, topic_name)\n",
    "            student.add_subtopic(subtopic)\n",
    "            subtopic = student.get_subtopic(subtopic.id)\n",
    "    else: # If student has no subtopics, create the subtopic object\n",
    "        subtopic = students.Subtopic(subtopic_name, grade, education, topic_name)\n",
    "        # connect subtopic object  to student object\n",
    "        student.add_subtopic(subtopic)\n",
    "        subtopic = student.get_subtopic(subtopic.id)\n",
    "\n",
    "    # ask the student a question\n",
    "    question = ask_question(student.name, subtopic, user_type,id_token,diff_level)\n",
    "    # print(question)\n",
    "    # receive answer,  calculate GPT answer, have a chance for the student to ask questions, evaluate student\n",
    "    metric_updates = receive_respond_and_update(question, student, subtopic)\n",
    "\n",
    "    # print(json.dumps(metric_updates, indent=2, sort_keys=True)) # print json\n",
    "    # # update the database's subtopic data\n",
    "    # print_line(1000)\n",
    "    subtopic.update_subtopic(metric_updates) \n",
    "    # print_line(1000)\n",
    "    # add the json mistakes update\n",
    "    # print(f\"\\n\\n student mistakes: {student.mistakes} \")\n",
    "    student.add_mistakes(metric_updates)\n",
    "    \n",
    "    # remove old object and add new object ( updates object if it already is in database)\n",
    "    if student.name in all_student_names:\n",
    "        StudentCollection.delete_student(student)\n",
    "    StudentCollection.add_student(student)\n",
    "    \n",
    "    # StudentCollection\n",
    "    # Ask the student if they want to be asked another question.\n",
    "    answer = input(\"Do you want another question? 'yes' or 'no' \")\n",
    "    while answer.lower() not in (\"yes\", \"no\"):\n",
    "        answer = input(\"Invalid input; Please enter 'yes' or 'no': \")\n",
    "    if answer == \"yes\":\n",
    "        student_learning()\n",
    "    else:\n",
    "        print(\"Thank you for using GPT Tutor! Have a great day.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T04:25:54.397995Z",
     "start_time": "2024-05-01T04:25:54.383979200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T04:25:54.816620800Z",
     "start_time": "2024-05-01T04:25:54.800564400Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that starts the learning process for either....\n",
    "    The student, or GPT\n",
    "    \"\"\"\n",
    "    # User: Uses GPT_Tutor to learn math\n",
    "    # Trainer: Testing GPT_Tutors knowledge ( ~ to MemPrompt paper)\n",
    "    valid = False # valid if you enter either 'user' or 'trainer'\n",
    "    while not valid:\n",
    "        print_line()\n",
    "        usage_type = input(\"Type 'user' or `1` if you use GPT to learn. \\nType 'trainer' or `2` if you want to train GPT_Tutor.\\nType anything else to exit the program\\n\")\n",
    "        usage_type = usage_type.lower() # makes sure letters are in lowercase\n",
    "        if usage_type == \"user\" or usage_type == \"1\":\n",
    "            valid = True\n",
    "            # Start the learning process.\n",
    "            student_learning()\n",
    "        elif usage_type == \"trainer\" or usage_type == \"2\":\n",
    "            valid = True\n",
    "            # Start GPT learning process\n",
    "            mem_prompt_learning()\n",
    "        else :\n",
    "            valid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if the script is being run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T04:57:18.514946800Z",
     "start_time": "2024-05-06T04:57:18.481545800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
