{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-01T12:39:48.490659900Z",
     "start_time": "2023-08-01T12:39:47.659992200Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "#TODO use dotenv file when you release final version\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer as ST\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T22:16:48.958504400Z",
     "start_time": "2023-08-01T22:16:48.926145100Z"
    }
   },
   "outputs": [],
   "source": [
    "# get syllabus\n",
    "# df1 = pd.read_csv('topics.csv')\n",
    "df2 = pd.read_csv('../data/GPT_tutor_topics(sub_topics_included).csv')\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T22:16:20.941313500Z",
     "start_time": "2023-08-01T22:16:20.919544200Z"
    }
   },
   "outputs": [],
   "source": [
    "# get key and model\n",
    "openai.api_key = \"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\"\n",
    "model_35 = \"gpt-3.5-turbo\"\n",
    "student_data_path = \"../data/students.json\"\n",
    "memory_path = \"../data/memory.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T20:25:18.903921Z",
     "start_time": "2023-08-01T20:25:18.885850500Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# getting external data\n",
    "def get_ext_data(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        database = json.load(file)\n",
    "    return database\n",
    "\n",
    "# post/update at the external data path\n",
    "def post_ext_data(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "# API responses\n",
    "def get_response(messages):\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model = model_35,\n",
    "        messages = messages,\n",
    "        temperature = 0 # make sure responses are deterministic/consistent\n",
    "    )\n",
    "    return res\n",
    "def get_response_text(messages):\n",
    "    res = get_response(messages)\n",
    "    return res['choices'][0]['message']['content']\n",
    "\n",
    "# creates one part of the message that you send to the GPT API for a response.\n",
    "# add brackets [] if you want to use this function to make a full message\n",
    "# System: 1\n",
    "# Assistant: 2\n",
    "# User: 3\n",
    "def get_simple_message_part(text, role_type):\n",
    "    role = None\n",
    "    if role_type == 1:\n",
    "        role = \"system\"\n",
    "    elif role_type == 2:\n",
    "        role = \"assistant\"\n",
    "    elif role_type == 3:\n",
    "        role = \"user\"\n",
    "\n",
    "    message_part = {\n",
    "        \"role\": role,\n",
    "        \"content\": text\n",
    "    }\n",
    "\n",
    "    return message_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.133654200Z",
     "start_time": "2023-08-01T12:58:11.116052700Z"
    }
   },
   "outputs": [],
   "source": [
    "# temporary helper functions\n",
    "def manual_level_reset(name, sub_topic, level):\n",
    "    # Get data from students.json\n",
    "    with open(student_data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Check if the student is in the database\n",
    "    students = data['students']\n",
    "    for student in students:\n",
    "        if name in student:\n",
    "            # Check if the student has a section for the given subtopic\n",
    "            sections = student[name]\n",
    "            for section in sections:\n",
    "                if section['sub_topic'] == sub_topic:\n",
    "                    # Reset the student's metrics\n",
    "                    section['proficiency_metrics'] = clear_metrics(section['proficiency_metrics'])\n",
    "                    section['level'] =  level\n",
    "                    section['questions_answered'] = [0, 0, 0, 0, 0]\n",
    "                    print(f\"{name}'s data metrics for '{sub_topic}' has been reset.\")\n",
    "                    break\n",
    "    print(\"level changed, database stats reset\")\n",
    "    # Write the updated data back to students.json\n",
    "    with open(student_data_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_student_subtopic_level(student, sub_topic):\n",
    "    # Read the JSON file\n",
    "    file_path = student_data_path\n",
    "    with open(file_path, \"r\") as file:\n",
    "        database = json.load(file)\n",
    "\n",
    "    # if we don't find the student, or the subtopic in the database, we will use the lowest level by default\n",
    "    default_level = 1\n",
    "\n",
    "    # Access the student's data from the database\n",
    "    student_data = None\n",
    "    for student_entry in database[\"students\"]:\n",
    "        if student in student_entry:\n",
    "            student_data = student_entry[student]\n",
    "            break\n",
    "\n",
    "    if student_data is not None:\n",
    "        # Find the sub-topic information for the student\n",
    "        sub_topic_data = None\n",
    "        for entry in student_data:\n",
    "            if entry[\"sub_topic\"] == sub_topic:\n",
    "                sub_topic_data = entry\n",
    "                break\n",
    "\n",
    "    # Check if student exists in the database\n",
    "    if student_data is None:\n",
    "        print(\"student is not in database. They will be start at Level 1, Proficiency 1\")\n",
    "        return default_level\n",
    "    # Check if sub-topic exists for the student\n",
    "    if sub_topic_data is None:\n",
    "        print(f\"student has not data for '{sub_topic}' in this database. They will be start at Level 1.\")\n",
    "        return default_level\n",
    "    else:\n",
    "        # Retrieve the level and proficiency scores\n",
    "        level = sub_topic_data[\"level\"]\n",
    "\n",
    "    # Return the level and proficiency scores\n",
    "    return level"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.265765800Z",
     "start_time": "2023-08-01T12:58:11.246464800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# helper functions for ask_question\n",
    "# database is currently 'students.json'\n",
    "# 1. make sure GPT only answers math questions\n",
    "def filter_answers():\n",
    "    message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"I am a math teacher for Grade K-12 in the United States. I am using the GPT API to help me answer my students' math questions. Please only answer my questions about math, and do not respond to any questions that are not about math.\"\n",
    "    }\n",
    "    return message\n",
    "# is_current_student: boolean\n",
    "def init_question(student, sub_topic):\n",
    "\n",
    "\n",
    "    # Prompt for level choice\n",
    "    #TODO integrate manually picking level\n",
    "    print(\" picking the level manually will only affect the type of questions you get\")\n",
    "    choice = input(\"Do you want to pick the level? (Y/N): \")\n",
    "\n",
    "    # If statement based on the choice\n",
    "    if choice.upper() == \"Y\":\n",
    "        valid = True\n",
    "        while valid:\n",
    "                level = int(input(\"Enter the level: \"))\n",
    "                if level > 5 or level < 1:\n",
    "                    print(\"Invalid level, pick again.\")\n",
    "                else:\n",
    "                    valid = False\n",
    "        # Reset student's level if needed\n",
    "        manual_level_reset(student, sub_topic, level)\n",
    "    else:\n",
    "        level = get_student_subtopic_level(student, sub_topic)\n",
    "    # criteria: tell GPT scales for proficiency and level\n",
    "\n",
    "    init = f\"Based on {student}'s database, the student's skill level for {sub_topic} is {level}. Please give {student} a test question based on {sub_topic} and follow up with a sentence like 'Explain how you got your answer'. Adjust the difficulty of the question based on his skill level and proficiency score. DO NOT include any other words. Do not put the answer in the prompt.\"\n",
    "    criteria = f\"Level is on a scale between 1 and 5, where 5 is the hardest level.\"\n",
    "\n",
    "    # combine criteria and message\n",
    "    message = f\"{init} {criteria}\"\n",
    "    init_crit = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": message\n",
    "    }\n",
    "\n",
    "    return init_crit\n",
    "# changes the format of the question GPT gives\n",
    "def question_formatting():\n",
    "    init = \"\"\"\n",
    "    This is the format that you should be using\n",
    "\n",
    "    \"\"\"\n",
    "    format = \"\"\"\n",
    "        Level 1 (Difficulty: Easy):\n",
    "        Subtract the following without regrouping (no borrowing):\n",
    "\n",
    "        1. 46-19\n",
    "\n",
    "    \"\"\"\n",
    "    level_meaning = \"\"\"\n",
    "    Remember the description that follows each Level\n",
    "\n",
    "    Level 1 (Difficulty: Easy):\n",
    "\n",
    "    Level 2 (Difficulty: Easy-Moderate):\n",
    "\n",
    "    Level 3 (Difficulty: Moderate):\n",
    "\n",
    "    Level 4 (Difficulty: Moderate-Hard):\n",
    "    Level 4 (Difficulty: Hard):\n",
    "\n",
    "    \"\"\"\n",
    "    formatting =  {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{init},{format}\"\n",
    "        }\n",
    "\n",
    "    level_meaning = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": level_meaning\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    return formatting, level_meaning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.411676Z",
     "start_time": "2023-08-01T12:58:11.400357500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# ask question to student\n",
    "def ask_question(student, sub_topic):\n",
    "    # make sure to only receive math answers and initialize the questions GPT will give\n",
    "    filter_subject = filter_answers()\n",
    "    filter_question = init_question(student, sub_topic)\n",
    "    formatting, level_meaning = question_formatting()\n",
    "    messages = [filter_subject, filter_question, formatting, level_meaning]\n",
    "    # print(messages)\n",
    "    tutor_question = get_response_text(messages)\n",
    "    # here we print out the question GPT gives the student\n",
    "    print(f\"{tutor_question}: \\n\\n\")\n",
    "    return tutor_question"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.643480600Z",
     "start_time": "2023-08-01T12:58:11.619632300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.789655Z",
     "start_time": "2023-08-01T12:58:11.768521200Z"
    }
   },
   "outputs": [],
   "source": [
    "# time: time it took the student to answer the question given from GPT\n",
    "# returns\n",
    "#   - the response the student gives\n",
    "#   - the time it takes to get a response form the studnet\n",
    "def get_student_timed_response():\n",
    "    start_time = time.time()\n",
    "\n",
    "    student_res = input() # response to question\n",
    "\n",
    "    end_time =  time.time()\n",
    "    end_time = end_time - start_time\n",
    "\n",
    "    return student_res, end_time\n",
    "def grade_student_response(question, student_answer, student,solve_time, sub_topic):\n",
    "    # take in the student's answer, and the topic\n",
    "    print(\"Answer: \\n\")\n",
    "    question_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a math tutor. The question that the user is answering is '{question}'.\"\n",
    "\n",
    "    }\n",
    "    answer_explained = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{student}'s answer is {student_answer}. Tell whether the student got the question correct and give and provide an explanation of the correct answer. Also explain where the student is incorrect\"\n",
    "    }\n",
    "    init_response_messages = [question_message,answer_explained]\n",
    "\n",
    "    answer_res = get_response_text(init_response_messages)\n",
    "    print(f\"{answer_res}\\n\\n\")\n",
    "\n",
    "    print(\"Evaluation: \\n\")\n",
    "    evaluation_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"The topic of the question is {sub_topic}. This is the question given to {student}: {question}. {student}'s answer is {student_answer}. This  is the answer you gave: {answer_res}.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"This I need you to evaluate {student}'s performance in terms of the following skill metrics: communication, interpretation, computation, conceptual, and the time taken to solve the question (it took the student {solve_time} seconds to complete the question. For each of these metrics, rate the skill out of 5, where 5 out of 5 is the best score. make sure to have your evaluation in outline format. Also give an explanation on how {student} did not get the highest marks \"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"at the end, give an average score based on the above metrics\"\n",
    "        }\n",
    "    ]\n",
    "    evaluation_res = get_response_text(evaluation_messages)\n",
    "    print(evaluation_res)\n",
    "    print(\" \\n\\n\")\n",
    "    metrics_scores = extract_metrics_scores(evaluation_res)\n",
    "\n",
    "    # print(metrics_scores)\n",
    "    return  metrics_scores\n",
    "def extract_metrics_scores(gpt_res):\n",
    "    instruction = f\"Extract the metric numbers:\\n\\n{gpt_res}\\n\\n---\\n. Answer this question in the form of a JSON file\"\n",
    "    example_text = \"\"\"\n",
    "        Evaluation of Allan's Performance:\n",
    "\n",
    "        1. Communication: 4/5\n",
    "           - Allan effectively communicated his answer and explanation in a clear and concise manner. However, there could have been more elaboration and clarity in his explanation.\n",
    "\n",
    "        2. Interpretation: 5/5\n",
    "           - Allan correctly interpreted the given equation and understood the objective of isolating x.\n",
    "\n",
    "        3. Computation: 5/5\n",
    "           - Allan correctly performed the necessary computation steps to solve the equation and obtained the correct answer.\n",
    "\n",
    "        4. Conceptual Understanding: 4/5\n",
    "           - Allan demonstrated a good understanding of the concept of isolating x in an equation. However, his explanation could have included more conceptual details to further enhance his understanding.\n",
    "\n",
    "        5. Time Taken: 5/5\n",
    "           - Allan was able to solve the question in a relatively short amount of time, taking only 20 seconds.\n",
    "\n",
    "        Average Score: (4 + 5 + 5 + 4 + 5) / 5 = 4.6/5\n",
    "\n",
    "        Explanation:\n",
    "        Allan's performance was generally strong across all skill metrics. He effectively communicated his answer and demonstrated a good understanding of the concept. However, his explanation could have been more detailed and comprehensive, which affected his score in the communication and conceptual understanding categories. Overall, Allan performed well and achieved a high average score of 4.6 out of 5.\n",
    "        \"\"\"\n",
    "    example_res = \"\"\"\n",
    "        {\n",
    "            \"proficiency_metrics\": {\n",
    "                \"proficiency_avg\": 4.6,\n",
    "                \"communication\": {\n",
    "                    \"score\": 4,\n",
    "                    \"related_mistakes\": [\"could have been more elaboration and clarity in his explanation.\"]\n",
    "                },\n",
    "                \"interpretation\": {\n",
    "                    \"score\": 5,\n",
    "                    \"related_mistakes\": []\n",
    "                },\n",
    "                \"computation\": {\n",
    "                    \"score\": 5,\n",
    "                    \"related_mistakes\": []\n",
    "                },\n",
    "                \"conceptual\": {\n",
    "                    \"score\": 4,\n",
    "                    \"related_mistakes\": [\"explanation could have included more conceptual details to further enhance his understanding\"]\n",
    "                },\n",
    "                \"time\": {\n",
    "                    \"score\": 5,\n",
    "                    \"seconds\": 20\n",
    "                }\n",
    "            }\n",
    "    }\n",
    "    \"\"\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": example_text},\n",
    "        {\"role\": \"assistant\", \"content\": example_res}]\n",
    "\n",
    "    # get JSON data in form of response string\n",
    "    metric_scores_string =  get_response_text(messages)\n",
    "\n",
    "    # make the string a JSON\n",
    "\n",
    "    metric_scores_json = eval(metric_scores_string)\n",
    "    return metric_scores_json\n",
    "\n",
    "    # # print(metric_scores_json)\n",
    "    # return metric_scores_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "# receive student cold ts answer, respond to their answer, and update their statistics\n",
    "def _receive_respond_and_update(question, student, sub_topic):\n",
    "    # Get the student's response and the time taken\n",
    "    student_res, solve_time = get_student_timed_response()\n",
    "\n",
    "    # Grade the student's response using the given question, student response, time, and sub_topic\n",
    "    gpt_res = grade_student_response(question, student_res, student, solve_time, sub_topic)\n",
    "\n",
    "    # Extract metric updates from the GPT response\n",
    "    metric_updates = extract_metrics_scores(gpt_res)\n",
    "\n",
    "    # Return the metric updates\n",
    "    return metric_updates\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:11.900666100Z",
     "start_time": "2023-08-01T12:58:11.888839700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:12.066030200Z",
     "start_time": "2023-08-01T12:58:12.047945100Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper functions for update_student_stats\n",
    "def update_scores_and_average(database_scores, new_score):\n",
    "\n",
    "    # the database stores the 3 most recent scores, so we will have to add our new_score and get rid of the old one\n",
    "\n",
    "    # add new score\n",
    "    # print(database_scores)\n",
    "    database_scores += [new_score]\n",
    "    # remove oldest score if there are more than 3 numbers in the list\n",
    "    if len(database_scores) > 3:\n",
    "        database_scores = database_scores[1:]\n",
    "\n",
    "# get the avg score and round to the last 2 decimals\n",
    "    new_avg_score = np.mean(database_scores)\n",
    "    new_avg_score = round(new_avg_score, 2)\n",
    "    return database_scores,new_avg_score\n",
    "def is_level_update_needed(overall_avg_stats):\n",
    "    bool = False\n",
    "    avg_score, recent_scores =  overall_avg_stats[\"avg_score\"],overall_avg_stats[\"recent_scores\"]\n",
    "    # if the avg score is 5, and we have 3 scores that make up the average, we need a level update\n",
    "    if avg_score == 5 and len(recent_scores) == 3:\n",
    "        bool = True\n",
    "    return bool\n",
    "def update_data(data,metrics_updates):\n",
    "\n",
    "    metrics, level,questions_answered  =  data[\"proficiency_metrics\"], data[\"level\"],data[\"questions_answered\"]\n",
    "\n",
    "    # update metrics\n",
    "    metrics = update_metrics(metrics,metrics_updates)\n",
    "    # update level and questions_answered\n",
    "\n",
    "    # each index of the array corresponds to the amount of questions answered a a certain level of difficulty\n",
    "    questions_answered[level-1] += 1\n",
    "\n",
    "    # if we have to upgrade to the next level, we get rid of our previous level's stats\n",
    "    if is_level_update_needed(metrics[\"overall_avg\"]):\n",
    "        if level > 5:\n",
    "            topic = data[\"topic\"]\n",
    "            print(f\"Congratulations, you have mastered the topic: {topic} the highest level available. Please pick another topic to learn\")\n",
    "        else:\n",
    "            level += 1\n",
    "            print(f\"Congratulations, you have moved up to Level {level}\")\n",
    "            metrics = clear_metrics(metrics) # remove previous level's data\n",
    "\n",
    "    return metrics,level,questions_answered\n",
    "def clear_metrics(old_metrics):\n",
    "    metrics = {\n",
    "        \"overall_avg\": {\n",
    "            \"avg_score\": 0,\n",
    "            \"recent_scores\": [\n",
    "            ]\n",
    "        },\n",
    "        \"communication\": {\n",
    "            \"avg_score\": 0,\n",
    "            \"related_mistakes\": [\n",
    "            ],\n",
    "            \"recent_scores\": [\n",
    "            ]\n",
    "        },\n",
    "        \"interpretation\": {\n",
    "            \"avg_score\": 0,\n",
    "            \"related_mistakes\": [],\n",
    "            \"recent_scores\": []\n",
    "        },\n",
    "        \"computation\": {\n",
    "            \"avg_score\": 5.0,\n",
    "            \"related_mistakes\": [],\n",
    "            \"recent_scores\": []\n",
    "        },\n",
    "        \"conceptual\": {\n",
    "            \"avg_score\": 0,\n",
    "            \"related_mistakes\": [],\n",
    "            \"recent_scores\": []\n",
    "        },\n",
    "        \"time\": {\n",
    "            \"avg_score\": 0,\n",
    "            \"avg_times\": None,\n",
    "            \"recent_times\": [\n",
    "            ],\n",
    "            \"recent_scores\": [\n",
    "            ]\n",
    "        }}\n",
    "    return metrics\n",
    "def update_metrics(metrics, metric_updates):\n",
    "\n",
    "    metric_types = ['overall_avg', 'communication', 'interpretation', 'computation', 'conceptual', 'time']\n",
    "\n",
    "    for metric_type in metric_types:\n",
    "        metric, metric_update = metrics[metric_type], metric_updates[metric_type]\n",
    "\n",
    "        if metric_type == 'overall_avg':\n",
    "            new_score = metric_update\n",
    "        else:\n",
    "            new_score = metric_update['score']\n",
    "            if 'related_mistakes' in metric_update:\n",
    "                metric['related_mistakes'] = metric_update['related_mistakes']\n",
    "            if 'seconds' in metric_update:\n",
    "                new_time = metric_update['seconds']\n",
    "                metric['recent_times'], metric['avg_times'] = update_scores_and_average(metric['recent_times'], new_time)\n",
    "\n",
    "        metric['recent_scores'], metric['avg_score'] = update_scores_and_average(metric['recent_scores'], new_score)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# updates the student metrics in the database\n",
    "def update_student_stats(name, sub_topic, metric_updates):\n",
    "    # Get data from students.json\n",
    "    with open(student_data_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Check if the student is in the database\n",
    "    students = data[\"students\"]\n",
    "    for student in students:\n",
    "        if name in student:\n",
    "            # Check if the student has a section for the given subtopic\n",
    "            sections = student[name]\n",
    "            for section in sections:\n",
    "                if section[\"sub_topic\"] == sub_topic:\n",
    "                    # Update the student's metrics\n",
    "                    section[\"proficiency_metrics\"], section[\"level\"],section[\"questions_answered\"], = update_data(section,metric_updates)\n",
    "                    print(f\"{name}'s data metrics for '{sub_topic}'has been updated \")\n",
    "                    break\n",
    "            else:\n",
    "                # If the student does not have data for that subtopic, add metric_updates\n",
    "                sections.append({\n",
    "                    \"sub_topic\": sub_topic,\n",
    "                    \"level\": 1,\n",
    "                    \"questions_answered\": [1,0,0,0,0],\n",
    "                    \"proficiency_metrics\": metric_updates\n",
    "                })\n",
    "                print(f\"{name}'s data metrics for '{sub_topic}'has been added \")\n",
    "\n",
    "            break\n",
    "    else:\n",
    "        # If the student is not found in the database, create a new entry\n",
    "        students.append({\n",
    "            name: [{\n",
    "                \"sub_topic\": sub_topic,\n",
    "                \"level\": 1,\n",
    "                \"questions_answered\": [1,0,0,0,0],\n",
    "                \"proficiency_metrics\": metric_updates\n",
    "            }]\n",
    "        })\n",
    "        print(f\"{name}'s data metrics for '{sub_topic}'has been added \")\n",
    "    # Write the updated data back to students.json\n",
    "    with open(student_data_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# asks student question, evaluates and updates their database\n",
    "def student_learning(student, sub_topic):\n",
    "    \"\"\"Asks the student a question and updates their stats.\"\"\"\n",
    "    question = ask_question(student, sub_topic)\n",
    "    metric_updates = _receive_respond_and_update(question, student, sub_topic)\n",
    "    update_student_stats(student, sub_topic, metric_updates)\n",
    "\n",
    "    # Ask the student if they want to be asked another question.\n",
    "    answer = input(\"Do you want another question? 'Yes' or 'No' \")\n",
    "    while answer not in (\"Yes\", \"No\"):\n",
    "        answer = input(\"Please enter 'Yes' or 'No': \")\n",
    "\n",
    "    if answer == \"Yes\":\n",
    "        student_learning(student, sub_topic)\n",
    "    else:\n",
    "        print(\"Thank you for using GPT Tutor! Have a great day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:12.197501300Z",
     "start_time": "2023-08-01T12:58:12.177401400Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO make a test portion\n",
    "#TODO Evaluating a student's response\n",
    "#TODO Evaluating AI/Student's Answer to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"The main function that starts the learning process.\"\"\"\n",
    "    student = input(\"Enter your name: \")\n",
    "    sub_topic = input(\"Enter the sub-topic you want to learn: \")\n",
    "\n",
    "    # Start the learning process.\n",
    "    student_learning(student, sub_topic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:12.346672200Z",
     "start_time": "2023-08-01T12:58:12.326076900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " picking the level manually will only affect the type of questions you get\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'student_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Check if the script is being run directly\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 7\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m sub_topic \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnter the sub-topic you want to learn: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Start the learning process.\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[43mstudent_learning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_topic\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 152\u001B[0m, in \u001B[0;36mstudent_learning\u001B[1;34m(student, sub_topic)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstudent_learning\u001B[39m(student, sub_topic):\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;124;03m\"\"\"Asks the student a question and updates their stats.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 152\u001B[0m     question \u001B[38;5;241m=\u001B[39m \u001B[43mask_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_topic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    153\u001B[0m     metric_updates \u001B[38;5;241m=\u001B[39m _receive_respond_and_update(question, student, sub_topic)\n\u001B[0;32m    154\u001B[0m     update_student_stats(student, sub_topic, metric_updates)\n",
      "Cell \u001B[1;32mIn[12], line 5\u001B[0m, in \u001B[0;36mask_question\u001B[1;34m(student, sub_topic)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mask_question\u001B[39m(student, sub_topic):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# make sure to only receive math answers and initialize the questions GPT will give\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     filter_subject \u001B[38;5;241m=\u001B[39m filter_answers()\n\u001B[1;32m----> 5\u001B[0m     filter_question \u001B[38;5;241m=\u001B[39m \u001B[43minit_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_topic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     formatting, level_meaning \u001B[38;5;241m=\u001B[39m question_formatting()\n\u001B[0;32m      7\u001B[0m     messages \u001B[38;5;241m=\u001B[39m [filter_subject, filter_question, formatting, level_meaning]\n",
      "Cell \u001B[1;32mIn[11], line 31\u001B[0m, in \u001B[0;36minit_question\u001B[1;34m(student, sub_topic)\u001B[0m\n\u001B[0;32m     29\u001B[0m     manual_level_reset(student, sub_topic, level)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 31\u001B[0m     level \u001B[38;5;241m=\u001B[39m \u001B[43mget_student_subtopic_level\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msub_topic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# criteria: tell GPT scales for proficiency and level\u001B[39;00m\n\u001B[0;32m     34\u001B[0m init \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBased on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms database, the student\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms skill level for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msub_topic\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlevel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please give \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m a test question based on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msub_topic\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and follow up with a sentence like \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExplain how you got your answer\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Adjust the difficulty of the question based on his skill level and proficiency score. DO NOT include any other words. Do not put the answer in the prompt.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m, in \u001B[0;36mget_student_subtopic_level\u001B[1;34m(student, sub_topic)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_student_subtopic_level\u001B[39m(student, sub_topic):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# Read the JSON file\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     file_path \u001B[38;5;241m=\u001B[39m \u001B[43mstudent_data_path\u001B[49m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m      5\u001B[0m         database \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'student_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if the script is being run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:15.909091800Z",
     "start_time": "2023-08-01T12:58:13.135815300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n",
    "# Example metric updates\n",
    "# updated_metrics = {\n",
    "#     \"overall_avg\": 4,\n",
    "#     \"communication\": {\n",
    "#         \"score\": 4,\n",
    "#         \"related_mistakes\": [\"needs to provide more examples\"]\n",
    "#     },\n",
    "#     \"interpretation\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\": []\n",
    "#     },\n",
    "#     \"computation\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\": []\n",
    "#     },\n",
    "#     \"conceptual\": {\n",
    "#         \"score\": 4,\n",
    "#         \"related_mistakes\":  [\"forgot to add 1\", \"forgot to simplify\"]\n",
    "#     },\n",
    "#     \"time\": {\n",
    "#         \"score\": 2,\n",
    "#         \"seconds\": 300\n",
    "#     }\n",
    "# }\n",
    "# updated_metrics_2 = {\n",
    "#     \"overall_avg\": 5,\n",
    "#     \"communication\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\": []\n",
    "#     },\n",
    "#     \"interpretation\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\": []\n",
    "#     },\n",
    "#     \"computation\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\": []\n",
    "#     },\n",
    "#     \"conceptual\": {\n",
    "#         \"score\": 5,\n",
    "#         \"related_mistakes\":  []\n",
    "#     },\n",
    "#     \"time\": {\n",
    "#         \"score\": 5,\n",
    "#         \"seconds\": 30\n",
    "#     }\n",
    "# }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:18.522333700Z",
     "start_time": "2023-08-01T12:58:18.506819800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# converts question's format into python formatting\n",
    "def question_python_notation(question):\n",
    "    system_text = \"\"\"\n",
    "    make the following questions in python notation:\n",
    "\n",
    "    Here is an example\"\n",
    "\n",
    "    User: \"What is 0.9^6\" in python notation\n",
    "\n",
    "    Assistant: \"What is 0.9**6\"\n",
    "\n",
    "    User: \"What is 0.9 raised to the power of 6\" in python notation\n",
    "\n",
    "    Assistant: \"What is 0.9**6\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    prompt_text = question\n",
    "    system_msg = get_simple_message_part(system_text,1)\n",
    "    prompt_msg = get_simple_message_part(prompt_text,3)\n",
    "    messages = [system_msg,prompt_msg]\n",
    "    converted_notation = get_response_text(messages)\n",
    "    # print( converted_notation)\n",
    "    return  converted_notation\n",
    "\n",
    "# using question given and answer given in python, make GPT backtrack to find a solution that gets the answer\n",
    "def backtrack_to_explanation(question, answer):\n",
    "    # to make sure that are answers are accurate and consistant, we make sure that GPT receives the answers in python notation\n",
    "    converted_question = question_python_notation(question)\n",
    "\n",
    "    system_text = \"You will be given a question, and an answer to a question. It is your job to explain the correct way how to get from the question to the answer step by step.  \"\n",
    "    prompt_text = f\"the question is {converted_question}, the answer is {answer}\"\n",
    "\n",
    "    system_msg = get_simple_message_part(system_text,1)\n",
    "    prompt_msg = get_simple_message_part(prompt_text,3)\n",
    "\n",
    "    messages = [system_msg,prompt_msg]\n",
    "    explanation = get_response_text(messages)\n",
    "    print(explanation)\n",
    "    return explanation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:18.968064Z",
     "start_time": "2023-08-01T12:58:18.957697700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#### TRYING TO run python code to get question right\n",
    "\n",
    "#TODO the use of \"exec could possible be a security risk\n",
    "def question_to_code_block(question):\n",
    "    message = [\n",
    "        {\n",
    "            # the code needed for \"exec\" to work need specific formatting\n",
    "            # make sure GPT will only give answers that are executable in python\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            When asked anything, the answer will always be in python code, using a function. no other comments. make sure to call the function\n",
    "\n",
    "            Example\n",
    "\n",
    "            user: what is 0.9**6\n",
    "\n",
    "            assistant:\n",
    "\n",
    "            def test()\n",
    "                value = 0.9**6\n",
    "                ans = f\"the answer to the question is {value}\"\n",
    "                return ans\n",
    "            result = test()\n",
    "            \"\"\"\n",
    "            # \"When asked anything, the answer will always be in python code, using a function. no other comments. use a print statement when calling the function\"\n",
    "            # \"When asked anything, the answer will always be in python code, using a function. no other comments, You are allowed to use external libraries such as numpy, scipy, etc\"\n",
    "\n",
    "            # \"When asked anything, the answer will always be in python code. no other comments, just python code. make sure the return is in a print statement\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": question\n",
    "        }\n",
    "\n",
    "    ]\n",
    "    code_block = get_response_text(message)\n",
    "    return code_block\n",
    "# This function takes a code block as input and returns the value given when the python code is executed\n",
    "def code_block_to_variable(code_block):\n",
    "    # Create an empty dictionary to store the variables that are created in the executed code.\n",
    "    data = {}\n",
    "\n",
    "    # Execute the code block and store the results in the dictionary `data`.\n",
    "    exec(code_block, None, data)\n",
    "\n",
    "    # Get the value of the variable `result` from the dictionary `data`.\n",
    "    var = data[\"result\"]\n",
    "\n",
    "    return var\n",
    "\n",
    "def solve_simple_math(question):\n",
    "    code_block = question_to_code_block(question)\n",
    "    print(f\"python code: {code_block}\")\n",
    "    ans = code_block_to_variable(code_block)\n",
    "    # convert answer into string\n",
    "    print(f\"answer: {ans}\")\n",
    "    return ans\n",
    "\n",
    "def explain_simple_math(question):\n",
    "    ans = solve_simple_math(question)\n",
    "    # print(f\" this is {str(ans)}\")\n",
    "    explanation = backtrack_to_explanation(question,ans)\n",
    "    # return explanation\n",
    "    return ans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:19.395993700Z",
     "start_time": "2023-08-01T12:58:19.376179700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# gets only the \"Question\" keys of JSON\n",
    "def get_memory_questions(filler_questions):\n",
    "    data = [question[\"Question\"] for question in filler_questions]\n",
    "    return data\n",
    "\n",
    "# Generate sentence embeddings for all the keys in the JSON file.\n",
    "# does cosine similarity for Questions ONLY\n",
    "# returns question and similarity score\n",
    "def find_most_similar_question(query):\n",
    "    model = ST('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Load the JSON file.\n",
    "    data = get_ext_data(memory_path)\n",
    "\n",
    "    # Preprocess the query to all lowercase.\n",
    "    query = query.lower()\n",
    "\n",
    "    # embed the query\n",
    "    query_embed = model.encode(query)\n",
    "\n",
    "    # Get all of the questions that are in the database\n",
    "\n",
    "    questions = get_memory_questions(data)\n",
    "\n",
    "    # embed the memory's questions into vector representation\n",
    "\n",
    "    memory_embeds = model.encode(questions)\n",
    "\n",
    "    # calculate the cosine similarity of each embed from memory compared to the query embed\n",
    "    cos_sim = cos([query_embed], memory_embeds)\n",
    "\n",
    "    # most_similar_question = np.max(cos_sim)\n",
    "# get the index of the question with the highest similarity score\n",
    "    most_similar_question_index = np.argmax(cos_sim)\n",
    "\n",
    "    most_similar_question = questions[most_similar_question_index]\n",
    "\n",
    "    return most_similar_question"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:11:48.996986300Z",
     "start_time": "2023-08-01T23:11:48.968828400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:11:51.644500300Z",
     "start_time": "2023-08-01T23:11:51.612115Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_feedback(question):\n",
    "    pass\n",
    "\n",
    "# creates prompt with question and feedback if it exists\n",
    "def create_prompt(question, feedback):\n",
    "    # make the question and feedback into one prompt\n",
    "    question_msg = get_simple_message_part(question,3)\n",
    "    set_up_msg = \"user feedback:\"\n",
    "    prepare_for_feedback = get_simple_message_part(set_up_msg,3)\n",
    "    feedback_msg = get_simple_message_part(feedback,3)\n",
    "\n",
    "    prompt = [question_msg, set_up_msg, feedback_msg]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    ans = get_response_text(prompt)\n",
    "    return ans\n",
    "\n",
    "def user_feedback(answer, reasoning):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "get_ext_data(memory_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "create_prompt()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T22:17:27.633358Z",
     "start_time": "2023-08-01T22:17:27.617371700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# backtrack_to_explanation(\"Find x when 2x = 10\", \"x = 5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:20.389926600Z",
     "start_time": "2023-08-01T12:58:20.365094800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\n",
    "t_1 = \"what is 0.9**6\"\n",
    "t_2 = \"2x + y = z; z = 0, y = -12, find x, y and z\"\n",
    "\n",
    "p_1 = get_simple_message_part(t_1,3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:59:51.332882300Z",
     "start_time": "2023-08-01T12:59:51.320275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.9 raised to the power of 6 is approximately 0.531441.'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer([p_1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:59:56.637685600Z",
     "start_time": "2023-08-01T12:59:55.081393100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python code: def solve_equations():\n",
      "    x = (z - y) / 2\n",
      "    y = -12\n",
      "    z = 0\n",
      "    return f\"The values of x, y, and z are {x}, {y}, and {z} respectively.\"\n",
      "\n",
      "result = solve_equations()\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'z' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mexplain_simple_math\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt_2\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[21], line 60\u001B[0m, in \u001B[0;36mexplain_simple_math\u001B[1;34m(question)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexplain_simple_math\u001B[39m(question):\n\u001B[1;32m---> 60\u001B[0m     ans \u001B[38;5;241m=\u001B[39m \u001B[43msolve_simple_math\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;66;03m# print(f\" this is {str(ans)}\")\u001B[39;00m\n\u001B[0;32m     62\u001B[0m     explanation \u001B[38;5;241m=\u001B[39m backtrack_to_explanation(question,ans)\n",
      "Cell \u001B[1;32mIn[21], line 54\u001B[0m, in \u001B[0;36msolve_simple_math\u001B[1;34m(question)\u001B[0m\n\u001B[0;32m     52\u001B[0m code_block \u001B[38;5;241m=\u001B[39m question_to_code_block(question)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcode_block\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 54\u001B[0m ans \u001B[38;5;241m=\u001B[39m \u001B[43mcode_block_to_variable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode_block\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# convert answer into string\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mans\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[21], line 44\u001B[0m, in \u001B[0;36mcode_block_to_variable\u001B[1;34m(code_block)\u001B[0m\n\u001B[0;32m     41\u001B[0m data \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Execute the code block and store the results in the dictionary `data`.\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m \u001B[43mexec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode_block\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# Get the value of the variable `result` from the dictionary `data`.\u001B[39;00m\n\u001B[0;32m     47\u001B[0m var \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m<string>:7\u001B[0m\n",
      "File \u001B[1;32m<string>:2\u001B[0m, in \u001B[0;36msolve_equations\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: local variable 'z' referenced before assignment"
     ]
    }
   ],
   "source": [
    "explain_simple_math(t_2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:58:35.003631400Z",
     "start_time": "2023-08-01T12:58:31.777094300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "\n",
    "model= ST(model_name)\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "cos([embeddings[1]], [embeddings[0]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T21:16:02.182854100Z",
     "start_time": "2023-08-01T21:16:02.170363300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def get_most_similar_key(query, json_file_path):\n",
    "    # Load Sentence Transformers model\n",
    "    model = ST('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Load the JSON file.\n",
    "    data = get_ext_data(json_file_path)\n",
    "\n",
    "    # Preprocess the query to all lowercase.\n",
    "    query = query.lower()\n",
    "\n",
    "    # Generate sentence embeddings for all the keys in the JSON file.\n",
    "    key_embeddings = model.encode([key.lower() for key in data.keys()])\n",
    "\n",
    "    # Generate the query embedding.\n",
    "    query_embedding = model.encode([query])[0]\n",
    "\n",
    "    # Calculate cosine similarity scores between the query and the keys.\n",
    "    similarity_scores = defaultdict(float)\n",
    "    for key, key_embedding in zip(data.keys(), key_embeddings):\n",
    "        similarity_scores[key] = cos(query_embedding, key_embedding)\n",
    "\n",
    "    # Find the most similar key to the query.\n",
    "    most_similar_key = max(similarity_scores, key=similarity_scores.get)\n",
    "\n",
    "    return most_similar_key\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T21:17:36.853887700Z",
     "start_time": "2023-08-01T21:17:35.834237700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 768)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T21:17:39.932908500Z",
     "start_time": "2023-08-01T21:17:39.911082500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:05:42.252978800Z",
     "start_time": "2023-08-01T23:05:41.986022700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_question_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:05:42.841834200Z",
     "start_time": "2023-08-01T23:05:42.823169400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:10:34.754106800Z",
     "start_time": "2023-08-01T23:10:34.735395700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.97463185, 1.        ])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [[4, 5, 6],[1,2,3]]\n",
    "\n",
    "max(cos([x],y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:00:55.533105700Z",
     "start_time": "2023-08-01T23:00:55.512601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.97463185, 1.        ])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cos([x],y)\n",
    "max(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T23:01:22.478218800Z",
     "start_time": "2023-08-01T23:01:22.432974200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
